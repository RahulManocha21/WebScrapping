{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077171ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b5e9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WildFlower():\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    WildFlower = wait.until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[2]/div[3]/nav/ul/li[1]/a\")))  \n",
    "    WildFlower.click()\n",
    "    time.sleep(5)\n",
    "    driver.implicitly_wait(10)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')   \n",
    "    div = soup.find_all('div', id='wildflowers_subcategories')\n",
    "    subCategory=[]\n",
    "    for i in div:\n",
    "            anchor_tags = i.find_all('a')\n",
    "            for anchor_tag in anchor_tags:\n",
    "                href_value = anchor_tag.get('href')\n",
    "                subCategory.append(href_value)\n",
    "    return subCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fa6ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FlowerSeed():\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    FlowerSeed = wait.until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[2]/div[3]/nav/ul/li[2]/a\")))  \n",
    "    FlowerSeed.click()\n",
    "    time.sleep(5)\n",
    "    driver.implicitly_wait(10)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    div = soup.find_all('div', class_='text_root__bzY2i pb-text-root')\n",
    "    subCategory=[]\n",
    "    for i in div:\n",
    "        anchor_tags = i.find_all('a')\n",
    "        for anchor_tag in anchor_tags:\n",
    "            href_value = anchor_tag.get('href')\n",
    "            if href_value and 'https://www.americanmeadows.com/category' in href_value:\n",
    "                subCategory.append(href_value)     \n",
    "    return subCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a2c13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_ProductLinks(numeric_part):\n",
    "    URL = driver.current_url + '?page={}'\n",
    "    for i in range(numeric_part):\n",
    "        CURL = URL.format(i+1)\n",
    "        driver.get(CURL)\n",
    "        time.sleep(3)\n",
    "        ProPage_Link = soup.find_all('a', class_='ProductTile_productimg-link__deS5f productimg-link')\n",
    "        for ProPage in ProPage_Link:\n",
    "            href_value = ProPage.get('href')\n",
    "            ProPage_links.append(href_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a278570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(Orignal_list):\n",
    "    unique_list = [item for index, item in enumerate(Orignal_list) if item not in Orignal_list[:index]]\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87046b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_info(URL):\n",
    "    driver.get(URL)\n",
    "    time.sleep(5)\n",
    "    driver.implicitly_wait(10)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    SKU_element = soup.find_all('div', class_='OverviewSection_feature-content__Yp3Nx feature-content')\n",
    "    product_name_element = soup.find('h1', class_='PDP_product-name__0qgud product-name')\n",
    "    Sub_pro_element = soup.find_all('span', class_='item-label')\n",
    "    regular_price_element = soup.find_all('span', class_='old-price line-through text-accents-7')\n",
    "    sale_price_element = soup.find_all('span', class_='product-price--value')\n",
    "    Rating_element = soup.find('div', class_='pr-snippet-rating-decimal')\n",
    "    Review_Count_element = soup.find('a', class_='pr-snippet-review-count')\n",
    "    for i in SKU_element:\n",
    "        SKU = i.text\n",
    "    product_name = product_name_element.text.strip()\n",
    "    regularPrice = [element.text.strip() for element in regular_price_element] if regular_price_element else None\n",
    "    salePrice = [element.text.strip() for element in sale_price_element] if sale_price_element else None\n",
    "    Rating = Rating_element.text.strip() if Rating_element else 'No Reviews'\n",
    "    ReviewCount= Review_Count_element.text.strip() if Review_Count_element else '0'\n",
    "    df =[]\n",
    "    if not Sub_pro_element:\n",
    "        paths = [\n",
    "        '/html/body/div[1]/div/main/div[3]/div[1]/div[2]/div/div[2]/div/div[5]/div[1]/span',\n",
    "        '/html/body/div[1]/div/main/div[3]/div[1]/div[2]/div/div[2]/div/div[4]/div[1]/span'\n",
    "        ]\n",
    "        for path in paths:\n",
    "            try:\n",
    "                Sub_pro_element = driver.find_element(By.XPATH, path)\n",
    "                Sub_pro = Sub_pro_element.text\n",
    "                # If the element is found, break out of the loop\n",
    "                break\n",
    "            except NoSuchElementException as e:\n",
    "                # Handle the exception, for example, print an error message\n",
    "                print(f\"Element not found for path {path}: {e}\")\n",
    "        else:\n",
    "            Sub_pro = None\n",
    "            print(\"All attempts failed. Element not found.\")\n",
    "            \n",
    "\n",
    "        row = {\n",
    "                'PageURL': URL,\n",
    "                'SKU' : SKU,\n",
    "                'ProductName': product_name,\n",
    "                'Sub_Product':Sub_pro,\n",
    "                'RegularPrice':regularPrice[0] if regularPrice is not None else None,\n",
    "                'SalePrice':salePrice[0],\n",
    "                'ReviewCount': ReviewCount,\n",
    "                'OverAllRating': Rating\n",
    "        }\n",
    "        df.append(row)\n",
    "\n",
    "    else:\n",
    "        Sub_pro = [element.text.strip() for element in Sub_pro_element]\n",
    "        for i in range(len(Sub_pro)):\n",
    "            row = {\n",
    "                'PageURL': URL,\n",
    "                'SKU' : SKU,\n",
    "                'ProductName': product_name,\n",
    "                'Sub_Product':Sub_pro[i],\n",
    "                'RegularPrice':regularPrice[i] if regularPrice is not None else None,\n",
    "                'SalePrice':salePrice[i],\n",
    "                'ReviewCount': ReviewCount,\n",
    "                'OverAllRating': Rating\n",
    "            }\n",
    "            df.append(row)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a236c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemap_urls(sitemap_url):\n",
    "    response = requests.get(sitemap_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        urls = [loc.text for loc in soup.find_all('loc')]\n",
    "        return urls\n",
    "    else:\n",
    "        print(f\"Failed to fetch sitemap. Status Code: {response.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0a333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul Manocha\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4436/4436 [00:00<00:00, 2241678.62it/s]\n"
     ]
    }
   ],
   "source": [
    "sitemap_url = 'https://www.americanmeadows.com/amisitemap.xml'\n",
    "sitemap_urls = get_sitemap_urls(sitemap_url)\n",
    "Products_URL= []\n",
    "for i in tqdm(sitemap_urls):\n",
    "    if 'https://www.americanmeadows.com/product' in i:\n",
    "        Products_URL.append(i)\n",
    "df = pd.DataFrame(Products_URL)\n",
    "df.to_csv('SitemapLinks.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30a9b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Web_URL = 'https://www.americanmeadows.com'\n",
    "allowed_user_agents = [\n",
    "    'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',\n",
    "    'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',\n",
    "    \n",
    "]\n",
    "ProPage_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bbd1005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "16\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "16\n",
      "20\n",
      "11\n",
      "5\n",
      "9\n",
      "8\n",
      "8\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "39\n",
      "9\n",
      "20\n",
      "5\n",
      "55\n",
      "5840\n",
      "166\n"
     ]
    }
   ],
   "source": [
    "user_agents = random.choice(allowed_user_agents)\n",
    "headers = {'User-Agent': user_agents}\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(Web_URL)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "####################################################################################################################################\n",
    "for i in WildFlower():\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')    \n",
    "    pagination = soup.find_all('a', class_='Pagination_nav-button__A1S2C')\n",
    "    a = [i.find('span', class_='sr-only') for i in pagination]\n",
    "    a = [i.text for i in a]\n",
    "    if len(a) == 1:\n",
    "        numeric_part = 1\n",
    "    else:\n",
    "        a = a[-2]  \n",
    "        numeric_part = ''.join(char for char in a if char.isdigit())\n",
    "        numeric_part = int(numeric_part)\n",
    "    print(numeric_part)\n",
    "    Get_ProductLinks(numeric_part)\n",
    "    \n",
    "####################################################################################################################################\n",
    "for i in FlowerSeed():\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')    \n",
    "    pagination = soup.find_all('a', class_='Pagination_nav-button__A1S2C')\n",
    "    a = [i.find('span', class_='sr-only') for i in pagination]\n",
    "    a = [i.text for i in a]\n",
    "    if len(a) == 1:\n",
    "        numeric_part = 1\n",
    "    else:\n",
    "        a = a[-2]  \n",
    "        numeric_part = ''.join(char for char in a if char.isdigit())\n",
    "        numeric_part = int(numeric_part)\n",
    "    print(numeric_part)\n",
    "    Get_ProductLinks(numeric_part)\n",
    "\n",
    "####################################################################################################################################\n",
    "wait = WebDriverWait(driver, 10)\n",
    "Perennials = wait.until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[2]/div[3]/nav/ul/li[3]/a\")))  \n",
    "Perennials.click()\n",
    "time.sleep(5)\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')    \n",
    "pagination = soup.find_all('a', class_='Pagination_nav-button__A1S2C')\n",
    "a = [i.find('span', class_='sr-only') for i in pagination]\n",
    "a = [i.text for i in a]\n",
    "if len(a) == 1:\n",
    "    numeric_part = 1\n",
    "else:\n",
    "    a = a[-2]  \n",
    "    numeric_part = ''.join(char for char in a if char.isdigit())\n",
    "    numeric_part = int(numeric_part)\n",
    "print(numeric_part)\n",
    "Get_ProductLinks(numeric_part)\n",
    "\n",
    "####################################################################################################################################\n",
    "PrePlannedGardens = wait.until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[2]/div[3]/nav/ul/li[4]/a\")))\n",
    "PrePlannedGardens.click()\n",
    "time.sleep(5)\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "pagination = soup.find_all('a', class_='Pagination_nav-button__A1S2C')\n",
    "a = [i.find('span', class_='sr-only') for i in pagination]\n",
    "a = [i.text for i in a]\n",
    "if len(a) == 1:\n",
    "    numeric_part = 1\n",
    "else:\n",
    "    a = a[-2]  \n",
    "    numeric_part = ''.join(char for char in a if char.isdigit())\n",
    "    numeric_part = int(numeric_part)\n",
    "print(w)\n",
    "Get_ProductLinks(numeric_part)\n",
    "\n",
    "####################################################################################################################################\n",
    "flowerBulbs = wait.until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[2]/div[3]/nav/ul/li[5]/a\")))\n",
    "flowerBulbs.click()\n",
    "time.sleep(5)\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "pagination = soup.find_all('a', class_='Pagination_nav-button__A1S2C')\n",
    "a = [i.find('span', class_='sr-only') for i in pagination]\n",
    "a = [i.text for i in a]\n",
    "if len(a) == 1:\n",
    "    numeric_part = 1\n",
    "else:\n",
    "    a = a[-2]  \n",
    "    numeric_part = ''.join(char for char in a if char.isdigit())\n",
    "    numeric_part = int(numeric_part)\n",
    "print(numeric_part)\n",
    "Get_ProductLinks(numeric_part)\n",
    "\n",
    "####################################################################################################################################\n",
    "Lawn = wait.until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[2]/div[3]/nav/ul/li[6]/a\")))\n",
    "Lawn.click()\n",
    "time.sleep(5)\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "pagination = soup.find_all('a', class_='Pagination_nav-button__A1S2C')\n",
    "a = [i.find('span', class_='sr-only') for i in pagination]\n",
    "a = [i.text for i in a]\n",
    "if len(a) == 1:\n",
    "    numeric_part = 1\n",
    "else:\n",
    "    a = a[-2]  \n",
    "    numeric_part = ''.join(char for char in a if char.isdigit())\n",
    "    numeric_part = int(numeric_part)\n",
    "print(numeric_part)\n",
    "Get_ProductLinks(numeric_part)\n",
    "\n",
    "####################################################################################################################################\n",
    "Sale = wait.until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[2]/div[3]/nav/ul/li[8]/a\")))\n",
    "Sale.click()\n",
    "time.sleep(5)\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "pagination = soup.find_all('a', class_='Pagination_nav-button__A1S2C')\n",
    "a = [i.find('span', class_='sr-only') for i in pagination]\n",
    "a = [i.text for i in a]\n",
    "if len(a) == 1:\n",
    "    numeric_part = 1\n",
    "else:\n",
    "    a = a[-2]  \n",
    "    numeric_part = ''.join(char for char in a if char.isdigit())\n",
    "    numeric_part = int(numeric_part)\n",
    "print(numeric_part)\n",
    "Get_ProductLinks(numeric_part)\n",
    "\n",
    "####################################################################################################################################\n",
    "print(len(ProPage_links))\n",
    "Unique_Links = unique(ProPage_links)\n",
    "print(len(Unique_Links))\n",
    "df = pd.DataFrame(Unique_Links)\n",
    "df.to_csv('Links.csv',index=False)\n",
    "\n",
    "Product_details=[]\n",
    "for i in Unique_Links:\n",
    "    URL = Web_URL + i\n",
    "    df = get_product_info(URL)\n",
    "    Product_details.extend(df)\n",
    "\n",
    "daf = pd.DataFrame(Product_details)\n",
    "daf.to_csv('AmericanMeadows.csv',index=False)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ec25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
