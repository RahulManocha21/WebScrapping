{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58d0f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b19a2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sitemap_urls(sitemap_url):\n",
    "    Pro=[]\n",
    "    for i in sitemap_url:\n",
    "        response = requests.get(i)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            urls = [loc.text for loc in soup.find_all('loc')]\n",
    "            Pro.extend(urls) \n",
    "        else:\n",
    "            print(f\"Failed to fetch sitemap. Status Code: {response.status_code}\")\n",
    "    return Pro     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb89b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unique(Orignal_list):\n",
    "    unique_list = [item for index, item in enumerate(Orignal_list) if item not in Orignal_list[:index]]\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c04e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrollWindow():\n",
    "    time.sleep(2)\n",
    "    initial_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "        driver.implicitly_wait(2) \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == initial_height:\n",
    "            break\n",
    "        initial_height = new_height\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "071d3ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_product_info(URL):\n",
    "    driver.get(URL)\n",
    "    scrollWindow()\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    try:\n",
    "        Product_element = soup.find('h1', class_=\"product__title\")\n",
    "        Product = Product_element.text\n",
    "    except:\n",
    "        Product_element = soup.find('h5', class_=\"product__title\")\n",
    "        Product = Product_element.text\n",
    "\n",
    "    Rating_element = soup.find('span',class_=\"avg-score font-color-gray-darker\")\n",
    "    Rating = Rating_element.text if Rating_element else '0'\n",
    "    ReviewCount_element = soup.find('span', class_=\"reviews-qa-label font-color-gray\")\n",
    "    ReviewCount = ReviewCount_element.text.split(',')[0] if ReviewCount_element else '0 Reviews'\n",
    "    df=[]\n",
    "    skipped = []\n",
    "    form_class =soup.find_all('div' , class_=\"selector-wrapper\") \n",
    "    def creating_row(Form=None, Style= None, Size= None):\n",
    "        try:\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            SKU_element=  soup.find('span', class_='current-sku')\n",
    "            SKU = SKU_element.text\n",
    "        except:\n",
    "            SKU_element = driver.find_element(By.XPATH, './/div[1]/div/div/div[3]/div[1]/div[1]/div/div/span/span')\n",
    "            SKU = SKU_element.text\n",
    "        try:\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            Price_element=  soup.find('span', class_='h4')\n",
    "            Price = Price_element.text\n",
    "        except:\n",
    "            Price_element = driver.find_element(By.XPATH, './/div[1]/div/div/div[3]/div[1]/div[2]/div/span/span[1]')\n",
    "            Price = Price_element.text\n",
    "        Variant = []\n",
    "        if Form is not None:\n",
    "            Variant.append(Form)\n",
    "        if Style is not None:\n",
    "            Variant.append(Style)\n",
    "        if Size is not None:\n",
    "            Variant.append(Size)\n",
    "        row={\n",
    "             'PageURL': URL, \n",
    "            'Product': Product,\n",
    "            'SKU': SKU,\n",
    "            'Variant': '|'.join(Variant),\n",
    "            'Price': Price,\n",
    "            'ReviewCount': ReviewCount,\n",
    "            'Rating': Rating\n",
    "        }\n",
    "        return row\n",
    "    if len(form_class)>=1:\n",
    "        for i in range(len(soup.find_all('div' , class_=\"selector-wrapper\")[0].find_all('input'))):\n",
    "            Form_element = driver.find_element(By.XPATH, './/div[2]/div[1]/fieldset/span[{}]/label/span'.format(i+1))\n",
    "            driver.execute_script(\"arguments[0].click();\", Form_element)\n",
    "            Form = Form_element.text\n",
    "            if len(form_class)>=2:\n",
    "                for j in range(len(soup.find_all('div' , class_=\"selector-wrapper\")[1].find_all('input'))):\n",
    "                    Size_element = driver.find_element(By.XPATH, './/div[2]/div[2]/fieldset/span[{}]/label/span'.format(j+1))\n",
    "                    driver.execute_script(\"arguments[0].click();\", Size_element)\n",
    "                    Size = Size_element.text\n",
    "                    if len(form_class)==3:\n",
    "                        for k in reversed(range(len(soup.find_all('div' , class_=\"selector-wrapper\")[2].find_all('input')))):\n",
    "                            Style_element = driver.find_element(By.XPATH, './/div[2]/div[3]/fieldset/span[{}]/label/span'.format(k+1))\n",
    "                            driver.execute_script(\"arguments[0].click();\", Style_element)\n",
    "                            Style = Style_element.text\n",
    "                            row = creating_row(Form=Form, Style= Style, Size=Size)\n",
    "                            df.append(row)\n",
    "                    else:\n",
    "                        row = creating_row(Form=Form, Style= None, Size= Size)\n",
    "                        df.append(row)\n",
    "                        \n",
    "            else:\n",
    "                row = creating_row(Form=Form, Style= None, Size= None)\n",
    "                df.append(row)\n",
    "    else:\n",
    "        row = creating_row(Form=None, Style= None, Size= None)\n",
    "        df.append(row)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4fb991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul Manocha\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sitemap_url = ['https://www.harrisseeds.com/sitemap_products_1.xml?from=8434003665&to=6592712900680',\n",
    "              'https://www.harrisseeds.com/sitemap_products_2.xml?from=6592753631304&to=6792758427720']\n",
    "sitemap_urls = get_sitemap_urls(sitemap_url)\n",
    "Pro_URL= []\n",
    "for i in sitemap_urls:\n",
    "    if 'https://www.harrisseeds.com/products' in i:\n",
    "        Pro_URL.append(i)\n",
    "df = pd.DataFrame(Pro_URL)\n",
    "df.to_csv('Links.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f500b817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allowed_user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Safari/14.1.2\",\n",
    "    \"Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbee95f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Product_details= []\n",
    "#Pro_URL.index('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a4cc62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.harrisseeds.com/products/06445-onion-plant-collection-1-1800'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ca420d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2908"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique(Pro_URL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75bb8755",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GettingData: 100%|██████████████████████████████████████████████████████████████████| 240/240 [27:33<00:00,  6.89s/URL]\n"
     ]
    }
   ],
   "source": [
    "user_agents = random.choice(allowed_user_agents)\n",
    "headers = {'User-Agent': user_agents}\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "chrome_options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "for i in tqdm(Pro_URL[2668:], desc=\"GettingData\", unit=\"URL\"):\n",
    "    df = get_product_info(i)\n",
    "    Product_details.extend(df)\n",
    "    #print(df)\n",
    "daf = pd.DataFrame(Product_details)\n",
    "daf.to_csv('HarrisSeed.csv',index=False)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1aa2ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "daf = pd.DataFrame(Product_details)\n",
    "daf.to_csv('HarrisSeeds.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b63a1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac4ada7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7649"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Product_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17992435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
