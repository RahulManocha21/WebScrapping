{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b186fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1887e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemap_urls(sitemap_url):\n",
    "    Pro=[]\n",
    "    for i in sitemap_url:\n",
    "        response = requests.get(i)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            urls = [loc.text for loc in soup.find_all('loc')]\n",
    "            Pro.extend(urls) \n",
    "        else:\n",
    "            print(f\"Failed to fetch sitemap. Status Code: {response.status_code}\")\n",
    "    return Pro     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df36a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(Orignal_list):\n",
    "    unique_list = [item for index, item in enumerate(Orignal_list) if item not in Orignal_list[:index]]\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de08458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrollWindow():\n",
    "    time.sleep(2)\n",
    "    initial_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "        driver.implicitly_wait(2) \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == initial_height:\n",
    "            break\n",
    "        initial_height = new_height\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ad40225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_info(URL):\n",
    "    driver.get(URL)\n",
    "    scrollWindow()\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    Product = soup.find('h1').text\n",
    "    ID = soup.find('span', class_=\"master-product-id\").text\n",
    "    df = []\n",
    "    for i in range(len(soup.find_all('span', class_=\"font-weight-bold\"))):\n",
    "        Varient_element = soup.find('ul', class_=\"sizecode-swatches sizecode-attribute d-flex pt-sm-1 flex-wrap\").find_all('li')[i].find_all('span')[0]\n",
    "        Varient = Varient_element.text\n",
    "        Stock_status_element = soup.find('ul', class_=\"sizecode-swatches sizecode-attribute d-flex pt-sm-1 flex-wrap\").find_all('li')[i].find_all('span')[1]\n",
    "        StockStatus = Stock_status_element.text\n",
    "        try:\n",
    "            RegularPrice = soup.find_all('span', class_=\"strike-through list\")[i].find('span').get('content')\n",
    "            SalePrice = soup.find_all('span', class_=\"sales red\").find('span')[i].get('content')\n",
    "            row = {\n",
    "                'ProductURL': URL,\n",
    "                'ProductName':Product,\n",
    "                'ProductID':ID,\n",
    "                'Varient':Varient,\n",
    "                'StockStatus':StockStatus,\n",
    "                'RegularPrice': '$'+ RegularPrice,\n",
    "                'SalePrice': '$'+ SalePrice\n",
    "            }\n",
    "            df.append(row)\n",
    "        except Exception as e:\n",
    "            Price = soup.find_all('span', class_='value')[i].get('content')\n",
    "            row = {\n",
    "                'ProductURL': URL,\n",
    "                'ProductName':Product,\n",
    "                'ProductID':ID,\n",
    "                'Varient':Varient,\n",
    "                'StockStatus':StockStatus,\n",
    "                'RegularPrice': None,\n",
    "                'SalePrice': '$'+ Price\n",
    "            }\n",
    "            df.append(row)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c523320",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Safari/14.1.2\",\n",
    "    \"Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ec6c3b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitemap_url = ['https://www.johnnyseeds.com/sitemap_0-product.xml']\n",
    "sitemap_urls = get_sitemap_urls(sitemap_url)\n",
    "Pro_URL= []\n",
    "for i in sitemap_urls:\n",
    "    if 'https://www.johnnyseeds.com/' in i and 'https://www.johnnyseeds.com/featured/sets-and-collections' not in i :\n",
    "        Pro_URL.append(i)\n",
    "df = pd.DataFrame(Pro_URL)\n",
    "df.to_csv('Links.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bdc08c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Product_details=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "64e596c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.johnnyseeds.com/tools-supplies/seed-starting-supplies/soil-blocking/standard-dibbles-soil-blocker-accessory-9527.500.html'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pro_URL.index('https://www.johnnyseeds.com/tools-supplies/clothing/hoodies/mens-zip-hoodie-pacific-l-johnnys-logo-apparel-6671.html')\n",
    "Pro_URL[3074]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "15c3d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = random.choice(allowed_user_agents)\n",
    "headers = {'User-Agent': user_agents}\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "chrome_options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "for i in tqdm(Pro_URL[3075:], desc=\"GettingData\", unit=\"URL\"):\n",
    "    df = get_product_info(i)\n",
    "    Product_details.extend(df)\n",
    "    print(df)\n",
    "daf = pd.DataFrame(Product_details)\n",
    "daf.to_csv('JohnySeed.csv',index=False)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f3ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
