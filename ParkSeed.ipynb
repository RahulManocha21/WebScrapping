{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c73b1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a6899a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sitemap_urls(sitemap_url):\n",
    "    Pro=[]\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    for i in sitemap_url:\n",
    "        response = requests.get(i, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            urls = [loc.text for loc in soup.find_all('loc')]\n",
    "            Pro.extend(urls) \n",
    "        else:\n",
    "            print(f\"Failed to fetch sitemap. Status Code: {response.status_code}\")\n",
    "    return Pro     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78594fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unique(Orignal_list):\n",
    "    unique_list = [item for index, item in enumerate(Orignal_list) if item not in Orignal_list[:index]]\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db3e29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrollWindow():\n",
    "    time.sleep(2)\n",
    "    initial_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "        driver.implicitly_wait(2) \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == initial_height:\n",
    "            break\n",
    "        initial_height = new_height\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d35dd20f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_product_info(URL):\n",
    "    df=[]\n",
    "    driver.get(URL)\n",
    "    scrollWindow()\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    ProductName = soup.find('h1').text\n",
    "    if ProductName != 'Our apologies.':\n",
    "        SubHeading = soup.find('div',class_=\"name\").text.replace('\\n','').replace('\\xa0','')\n",
    "        ID = soup.find('span', class_=\"listrakprodno\").text\n",
    "        try:\n",
    "            SalePrice = soup.find('span',class_='default_value').text\n",
    "            RegularPrice=None\n",
    "        except:\n",
    "            RegularPrice = soup.find('span',class_='was').text.strip()\n",
    "            SalePrice = soup.find('span',class_='now').text.strip()\n",
    "        try:\n",
    "            Rating = soup.find('div',class_='h3').text.replace('\\n','').split(':')[-1].split('/')[0]\n",
    "        except:\n",
    "            Rating = 0\n",
    "        ReviewCount = soup.find('div',class_='ratings-md ratings-inline').find('a').text.split('\\n')[-1]\n",
    "        if ReviewCount ==' Be the first to review this product':\n",
    "            ReviewCount = '0 Review'\n",
    "        row={\n",
    "            'PageURL':URL,\n",
    "            'ProductName':ProductName,\n",
    "            'SubHeading': SubHeading,\n",
    "            'ProductID' : ID,\n",
    "            'RegularPrice':RegularPrice,\n",
    "            'SalePrice':SalePrice,\n",
    "            'ReviewCount':ReviewCount,\n",
    "            'Rating':Rating\n",
    "        }\n",
    "        df.append(row)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f65972d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allowed_user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d191d89",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul Manocha\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sitemap_url = ['https://parkseed.com/sitemap_product1.xml']\n",
    "sitemap_urls = get_sitemap_urls(sitemap_url)\n",
    "Pro_URL= []\n",
    "for i in sitemap_urls:\n",
    "    if 'https://parkseed.com/' in i:\n",
    "        Pro_URL.append(i)\n",
    "df = pd.DataFrame(Pro_URL)\n",
    "df.to_csv('Links.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "059620dd-6c5b-45d4-bb68-7cd64227a998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "868"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pro_URL[432+301+85+50]\n",
    "432+301+85+50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "666c08b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Product_details=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bc29d17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GettingData: 100%|██████████| 1/1 [00:16<00:00, 16.59s/URL]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(Pro_URL[868:869], desc=\"GettingData\", unit=\"URL\"):\n",
    "    user_agents = random.choice(allowed_user_agents)\n",
    "    headers = {'User-Agent': user_agents}\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "    chrome_options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    df = get_product_info(i)\n",
    "    Product_details.extend(df)\n",
    "    driver.quit() \n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    #print(df)\n",
    "# daf = pd.DataFrame(Product_details)\n",
    "# daf.to_csv('ParkSeed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0c6f1aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7c5ca6b-dbfe-401b-b9e8-d5e9b425f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = random.choice(allowed_user_agents)\n",
    "headers = {'User-Agent': user_agents}\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "chrome_options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(Pro_URL[868])\n",
    "scrollWindow()\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8db9015-8907-47ba-aa4f-2f4e9a3ad85c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ProductName = soup.find('h1').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b3d8a6-c3dd-4e3d-80cd-63c442957ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6511ee-0d1c-4035-9e05-5ffcc227e2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
